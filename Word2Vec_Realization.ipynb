{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(text, window_size=2):\n",
    "\t# Удаляем все символы кроме a-z, @, и #\n",
    "\ttext = re.sub(r'[^a-z@# ]', '', text)    \n",
    "\t# Преобразуем текст в нижний регистр\n",
    "\ttext = text.lower()\n",
    "\t# Разбиваем по словам\n",
    "\ttokens = text.split()    \n",
    "\t# Формируем словарь уникальных слов\n",
    "\tvocab = set(tokens)\n",
    "\t# Формируем слова слов с указанием индекса  слова в словаре\n",
    "\tword_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\t# Формируем пары слов n-грамм\n",
    "\tdata = []\n",
    "\tfor i in range(len(tokens)):\n",
    "\t\tfor j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "\t\t\tif i != j:\n",
    "\t\t\t\tdata.append((tokens[i], tokens[j]))    \n",
    "\treturn data, word_to_ix, len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModelDataset(Dataset):\n",
    "\tdef __init__(self, data, word_to_ix):\n",
    "\t\tself.data = [(word_to_ix[center], word_to_ix[context]) for center, context in data]\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn torch.tensor(self.data[idx][0], dtype=torch.long), torch.tensor(self.data[idx][1], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSkipGramModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embedding_dim):\n",
    "\t\tsuper(Word2VecSkipGramModel, self).__init__()\n",
    "\t\tself.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\t\tself.activation_function = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, center_word_idx):\n",
    "\t\thidden_layer = self.embeddings(center_word_idx)\n",
    "\t\tout_layer = self.out_layer(hidden_layer)\n",
    "\t\tlog_probs = self.activation_function(out_layer)\n",
    "\t\treturn log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, word_to_ix, vocab_size, embedding_dim=50, epochs=10, batch_size=1):\n",
    "\t# Формируем набор данных\n",
    "\tdataset = SkipGramModelDataset(data, word_to_ix)\n",
    "\tdataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\t# модель\n",
    "\tmodel = Word2VecSkipGramModel(vocab_size, embedding_dim)\n",
    "\t# функция потерь\n",
    "\tloss_function = nn.NLLLoss()\n",
    "\t#  оптимизатор\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor center_word, context_word in dataloader:\n",
    "\t\t\tmodel.zero_grad()\n",
    "\t\t\tlog_probs = model(center_word)\n",
    "\t\t\tloss = loss_function(log_probs, context_word)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()            \n",
    "\t\t\ttotal_loss += loss.item()\t\t\t\n",
    "\t\tprint(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основная функция для вызова\n",
    "def train(data: str):\n",
    "\t# Гиперпараметры:\n",
    "\t# размер окна\n",
    "\twindow_size = 2\n",
    "\t# длина ембединга\n",
    "\tembedding_dim = 10\n",
    "\t# количество эпох обучения\n",
    "\tepochs = 5\n",
    "\t# размер батча\n",
    "\tbatch_size = 2\n",
    "\t\n",
    "\t# предобработка данных\n",
    "\tngramm_data, word_to_ix, vocab_size = prepare_data(data, window_size) \n",
    "\t# основной процесс формирование и обучения модели\n",
    "\tmodel = train_model(ngramm_data, word_to_ix, vocab_size, embedding_dim, epochs, batch_size)\n",
    "\t\n",
    "\t# # Извлекаем векторы слов из модели\n",
    "\tembeddings = model.embeddings.weight.data.numpy()\n",
    "\t# формируем словарь слов и их векторное представление\n",
    "\tix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\tw2v_dict = {ix_to_word[ix]: embeddings[ix] for ix in range(vocab_size)}\n",
    "\treturn w2v_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3617.666100502014\n",
      "Epoch 2, Loss: 3207.677946448326\n",
      "Epoch 3, Loss: 3012.711859345436\n",
      "Epoch 4, Loss: 2860.186472296715\n",
      "Epoch 5, Loss: 2734.4632539749146\n",
      "Epoch 6, Loss: 2638.899858355522\n",
      "Epoch 7, Loss: 2538.966754436493\n",
      "Epoch 8, Loss: 2465.6429120898247\n",
      "Epoch 9, Loss: 2404.75001847744\n",
      "Epoch 10, Loss: 2352.578776985407\n",
      "Epoch 11, Loss: 2316.4191286563873\n",
      "Epoch 12, Loss: 2283.9881920814514\n",
      "Epoch 13, Loss: 2258.787660598755\n",
      "Epoch 14, Loss: 2242.6005405187607\n",
      "Epoch 15, Loss: 2214.313904672861\n",
      "Epoch 16, Loss: 2193.749487325549\n",
      "Epoch 17, Loss: 2188.7517686486244\n",
      "Epoch 18, Loss: 2179.3011078238487\n",
      "Epoch 19, Loss: 2165.4264918863773\n",
      "Epoch 20, Loss: 2155.266880095005\n",
      "Epoch 21, Loss: 2145.6149440407753\n",
      "Epoch 22, Loss: 2142.541223704815\n",
      "Epoch 23, Loss: 2139.577820658684\n",
      "Epoch 24, Loss: 2122.148221999407\n",
      "Epoch 25, Loss: 2119.5181475281715\n",
      "Epoch 26, Loss: 2118.0613654851913\n",
      "Epoch 27, Loss: 2117.4751784205437\n",
      "Epoch 28, Loss: 2114.4236257076263\n",
      "Epoch 29, Loss: 2103.0918547809124\n",
      "Epoch 30, Loss: 2107.0966660380363\n"
     ]
    }
   ],
   "source": [
    "# Тестовые данные\n",
    "test_text = 'Captures Semantic Relationships: The skip-gram model effectively captures semantic relationships between words. It learns word embeddings that encode similar meanings and associations, allowing for tasks like word analogies and similarity calculations. Handles Rare Words: The skip-gram model performs well even with rare words or words with limited occurrences in the training data. It can generate meaningful representations for such words by leveraging the context in which they appear. Contextual Flexibility: The skip-gram model allows for flexible context definitions by using a window around each target word. This flexibility captures local and global word associations, resulting in richer semantic representations. Scalability: The skip-gram model can be trained efficiently on large-scale datasets due to its simplicity and parallelization potential. It can process vast amounts of text data to generate high-quality word embeddings.'\n",
    "\n",
    "w2v_dict = train(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_cbow(text: str, window_size=2):\n",
    "\ttext = re.sub(r'[^a-z@# ]', '', text.lower())    \n",
    "\ttokens = text.split()    \n",
    "\t\n",
    "\tvocab = set(tokens)\n",
    "\tword_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\t\n",
    "\tdata = []\n",
    "\tfor i in range(window_size, len(tokens) - window_size):\n",
    "\t\tcontext = [tokens[i - j - 1] for j in range(window_size)] + [tokens[i + j + 1] for j in range(window_size)]\n",
    "\t\ttarget = tokens[i]\n",
    "\t\tdata.append((context, target))\n",
    "\treturn data, word_to_ix, len(vocab)\t\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "\tdef __init__(self, data, word_to_ix):\n",
    "\t\tself.contexts = []\n",
    "\t\tself.targets = []\n",
    "\t\tfor context, target in data:\n",
    "\t\t\tindexed_context = [word_to_ix[word] for word in context]\n",
    "\t\t\tself.contexts.append(indexed_context)\n",
    "\t\t\tself.targets.append(word_to_ix[target])\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.targets)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# Возвращаем контекст и центральное слово как пару тензоров\n",
    "\t\treturn torch.tensor(self.contexts[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение архитектуры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecCBOWModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embedding_dim):\n",
    "\t\tsuper(Word2VecCBOWModel, self).__init__()\n",
    "\t\tself.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\t\tself.activation_function = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\tdef forward(self, center_word_idx):\n",
    "\t\thidden_layer = torch.mean(self.embeddings(center_word_idx), dim=1)\n",
    "\t\tout_layer = self.out_layer(hidden_layer)\n",
    "\t\tlog_probs = self.activation_function(out_layer)\n",
    "\t\treturn log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обновление функции обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cbow(data, word_to_ix, vocab_size, embedding_dim=50, epochs=10, batch_size=1):\n",
    "\tdataset = CBOWDataset(data, word_to_ix)\n",
    "\tdataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\t\n",
    "\tmodel = Word2VecCBOWModel(vocab_size, embedding_dim)\n",
    "\tloss_function = nn.NLLLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\t\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor context_words, target_word in dataloader:\n",
    "\t\t\tcontext_words = context_words  # Подготавливаем контекстные слова\n",
    "\t\t\tmodel.zero_grad()\n",
    "\t\t\tlog_probs = model(context_words)\n",
    "\t\t\tloss = loss_function(log_probs, target_word)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\tprint(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data: str):\n",
    "\twindow_size = 2\n",
    "\tembedding_dim = 10\n",
    "\tepochs = 5\n",
    "\tbatch_size = 2\n",
    "\t\n",
    "\tngramm_data, word_to_ix, vocab_size = prepare_data_cbow(data, window_size)    \n",
    "\tmodel = train_model_cbow(ngramm_data, word_to_ix, vocab_size, embedding_dim, epochs, batch_size)\n",
    "\t\n",
    "\t# # Извлекаем векторы слов из модели\n",
    "\tembeddings = model.embeddings.weight.data.numpy()\n",
    "\tix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\tw2v_dict = {ix_to_word[ix]: embeddings[ix] for ix in range(vocab_size)}\n",
    "\treturn w2v_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 282.7689628601074\n",
      "Epoch 2, Loss: 282.6620945930481\n",
      "Epoch 3, Loss: 282.55592036247253\n",
      "Epoch 4, Loss: 282.4492983818054\n",
      "Epoch 5, Loss: 282.34387159347534\n"
     ]
    }
   ],
   "source": [
    "test_text = 'Captures Semantic Relationships: The skip-gram model effectively captures semantic relationships between words. It learns word embeddings that encode similar meanings and associations, allowing for tasks like word analogies and similarity calculations. Handles Rare Words: The skip-gram model performs well even with rare words or words with limited occurrences in the training data. It can generate meaningful representations for such words by leveraging the context in which they appear. Contextual Flexibility: The skip-gram model allows for flexible context definitions by using a window around each target word. This flexibility captures local and global word associations, resulting in richer semantic representations. Scalability: The skip-gram model can be trained efficiently on large-scale datasets due to its simplicity and parallelization potential. It can process vast amounts of text data to generate high-quality word embeddings.'\n",
    "\n",
    "w2v_dict = train(test_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
